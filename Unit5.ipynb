{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5d0ed6c-7284-4c81-bacb-e3945dcac91b",
   "metadata": {},
   "source": [
    "# Unit-5 Huggingface Audio course\n",
    "\n",
    "Building Dhievi fine-tuned whisper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38c1228-9d93-49aa-aaf1-456e3a70470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -Uqq librosa soundfile transformers evaluate datasets jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f97ac-5e95-461d-8d45-12af17aa9cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "from datasets import load_dataset, DatasetDict, Audio\n",
    "from evaluate import load\n",
    "from IPython.display import Audio\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "from transformers.models.whisper.tokenization_whisper import TO_LANGUAGE_CODE\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2586647d-f8a0-4ba9-be99-545bff9900c5",
   "metadata": {},
   "source": [
    "## datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c8b277-9a5a-4a48-888d-e9d7c2c7d7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset common_voice_13_0 (/home/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/dv/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055)\n"
     ]
    }
   ],
   "source": [
    "common_voice_test = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e6142c-d497-4397-b605-2e055836c6db",
   "metadata": {},
   "source": [
    "## Native performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c7a0be-ac35-4073-b30e-a8f12c163de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    torch_dtype = torch.float16\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    torch_dtype = torch.float32\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-tiny\",\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cf37a6-da62-4753-b2b0-f5a9f5d05dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2212/2212 [05:17<00:00,  6.96it/s]\n"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "\n",
    "# run streamed inference\n",
    "for prediction in tqdm(\n",
    "    pipe(\n",
    "        KeyDataset(common_voice_test, \"audio\"),\n",
    "        max_new_tokens=128,\n",
    "        generate_kwargs={\"task\": \"transcribe\"},\n",
    "        batch_size=32,\n",
    "    ),\n",
    "    total=len(common_voice_test),\n",
    "):\n",
    "    all_predictions.append(prediction[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536fa47e-ab3a-4076-8698-a54ccfddab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463a1251-d328-4b01-9749-8513bfa3133e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148.7638414931402"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wer_ortho = 100 * wer_metric.compute(\n",
    "    references=common_voice_test[\"sentence\"], predictions=all_predictions\n",
    ")\n",
    "wer_ortho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86028c5a-9167-4113-a2b6-23636532dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = BasicTextNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c25a09-caad-422c-8f05-b633ac19b706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109.39597315436242"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute normalised WER\n",
    "all_predictions_norm = [normalizer(pred) for pred in all_predictions]\n",
    "all_references_norm = [normalizer(label) for label in common_voice_test[\"sentence\"]]\n",
    "\n",
    "# filtering step to only evaluate the samples that correspond to non-zero references\n",
    "all_predictions_norm = [\n",
    "    all_predictions_norm[i]\n",
    "    for i in range(len(all_predictions_norm))\n",
    "    if len(all_references_norm[i]) > 0\n",
    "]\n",
    "all_references_norm = [\n",
    "    all_references_norm[i]\n",
    "    for i in range(len(all_references_norm))\n",
    "    if len(all_references_norm[i]) > 0\n",
    "]\n",
    "\n",
    "wer = 100 * wer_metric.compute(\n",
    "    references=all_references_norm, predictions=all_predictions_norm\n",
    ")\n",
    "\n",
    "wer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec5650-23e5-4f5a-9090-3b3be75975fc",
   "metadata": {},
   "source": [
    "## Fine-tuning whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6624f1b8-f730-4505-9ffb-7b9023da7b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset common_voice_13_0 (/home/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/dv/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055)\n",
      "Found cached dataset common_voice_13_0 (/home/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/dv/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
      "        num_rows: 4904\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
      "        num_rows: 2212\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"train+validation\"\n",
    ")\n",
    "common_voice[\"test\"] = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"test\"\n",
    ")\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03ea9d0-fd95-4f8c-8b57-d972eeec471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice = common_voice.select_columns([\"audio\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e3a227-6cdc-4b6d-bbba-d721cf06fc92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': 'en',\n",
       " 'chinese': 'zh',\n",
       " 'german': 'de',\n",
       " 'spanish': 'es',\n",
       " 'russian': 'ru',\n",
       " 'korean': 'ko',\n",
       " 'french': 'fr',\n",
       " 'japanese': 'ja',\n",
       " 'portuguese': 'pt',\n",
       " 'turkish': 'tr',\n",
       " 'polish': 'pl',\n",
       " 'catalan': 'ca',\n",
       " 'dutch': 'nl',\n",
       " 'arabic': 'ar',\n",
       " 'swedish': 'sv',\n",
       " 'italian': 'it',\n",
       " 'indonesian': 'id',\n",
       " 'hindi': 'hi',\n",
       " 'finnish': 'fi',\n",
       " 'vietnamese': 'vi',\n",
       " 'hebrew': 'he',\n",
       " 'ukrainian': 'uk',\n",
       " 'greek': 'el',\n",
       " 'malay': 'ms',\n",
       " 'czech': 'cs',\n",
       " 'romanian': 'ro',\n",
       " 'danish': 'da',\n",
       " 'hungarian': 'hu',\n",
       " 'tamil': 'ta',\n",
       " 'norwegian': 'no',\n",
       " 'thai': 'th',\n",
       " 'urdu': 'ur',\n",
       " 'croatian': 'hr',\n",
       " 'bulgarian': 'bg',\n",
       " 'lithuanian': 'lt',\n",
       " 'latin': 'la',\n",
       " 'maori': 'mi',\n",
       " 'malayalam': 'ml',\n",
       " 'welsh': 'cy',\n",
       " 'slovak': 'sk',\n",
       " 'telugu': 'te',\n",
       " 'persian': 'fa',\n",
       " 'latvian': 'lv',\n",
       " 'bengali': 'bn',\n",
       " 'serbian': 'sr',\n",
       " 'azerbaijani': 'az',\n",
       " 'slovenian': 'sl',\n",
       " 'kannada': 'kn',\n",
       " 'estonian': 'et',\n",
       " 'macedonian': 'mk',\n",
       " 'breton': 'br',\n",
       " 'basque': 'eu',\n",
       " 'icelandic': 'is',\n",
       " 'armenian': 'hy',\n",
       " 'nepali': 'ne',\n",
       " 'mongolian': 'mn',\n",
       " 'bosnian': 'bs',\n",
       " 'kazakh': 'kk',\n",
       " 'albanian': 'sq',\n",
       " 'swahili': 'sw',\n",
       " 'galician': 'gl',\n",
       " 'marathi': 'mr',\n",
       " 'punjabi': 'pa',\n",
       " 'sinhala': 'si',\n",
       " 'khmer': 'km',\n",
       " 'shona': 'sn',\n",
       " 'yoruba': 'yo',\n",
       " 'somali': 'so',\n",
       " 'afrikaans': 'af',\n",
       " 'occitan': 'oc',\n",
       " 'georgian': 'ka',\n",
       " 'belarusian': 'be',\n",
       " 'tajik': 'tg',\n",
       " 'sindhi': 'sd',\n",
       " 'gujarati': 'gu',\n",
       " 'amharic': 'am',\n",
       " 'yiddish': 'yi',\n",
       " 'lao': 'lo',\n",
       " 'uzbek': 'uz',\n",
       " 'faroese': 'fo',\n",
       " 'haitian creole': 'ht',\n",
       " 'pashto': 'ps',\n",
       " 'turkmen': 'tk',\n",
       " 'nynorsk': 'nn',\n",
       " 'maltese': 'mt',\n",
       " 'sanskrit': 'sa',\n",
       " 'luxembourgish': 'lb',\n",
       " 'myanmar': 'my',\n",
       " 'tibetan': 'bo',\n",
       " 'tagalog': 'tl',\n",
       " 'malagasy': 'mg',\n",
       " 'assamese': 'as',\n",
       " 'tatar': 'tt',\n",
       " 'hawaiian': 'haw',\n",
       " 'lingala': 'ln',\n",
       " 'hausa': 'ha',\n",
       " 'bashkir': 'ba',\n",
       " 'javanese': 'jw',\n",
       " 'sundanese': 'su',\n",
       " 'burmese': 'my',\n",
       " 'valencian': 'ca',\n",
       " 'flemish': 'nl',\n",
       " 'haitian': 'ht',\n",
       " 'letzeburgesch': 'lb',\n",
       " 'pushto': 'ps',\n",
       " 'panjabi': 'pa',\n",
       " 'moldavian': 'ro',\n",
       " 'moldovan': 'ro',\n",
       " 'sinhalese': 'si',\n",
       " 'castilian': 'es'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TO_LANGUAGE_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f28fcc2-3607-4c8a-a5b0-5fa7b7768798",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\n",
    "    \"openai/whisper-tiny\", language=\"sinhalese\", task=\"transcribe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e4ab46-02c6-4694-90c9-3b31fa58c2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': Audio(sampling_rate=48000, mono=True, decode=True, id=None),\n",
       " 'sentence': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba92096-c0aa-4ddc-98a1-716c05124d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'sentence'],\n",
       "    num_rows: 4904\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b522f3-427e-43a3-a015-6d7d69588892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_rate = processor.feature_extractor.sampling_rate\n",
    "sampling_rate\n",
    "# common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00504bb-ce6d-4968-ab30-386af8386d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0membed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mautoplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0melement_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Create an audio object.\n",
       "\n",
       "When this object is returned by an input cell or passed to the\n",
       "display function, it will result in Audio controls being displayed\n",
       "in the frontend (only works in the notebook).\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data : numpy array, list, unicode, str or bytes\n",
       "    Can be one of\n",
       "\n",
       "      * Numpy 1d array containing the desired waveform (mono)\n",
       "      * Numpy 2d array containing waveforms for each channel.\n",
       "        Shape=(NCHAN, NSAMPLES). For the standard channel order, see\n",
       "        http://msdn.microsoft.com/en-us/library/windows/hardware/dn653308(v=vs.85).aspx\n",
       "      * List of float or integer representing the waveform (mono)\n",
       "      * String containing the filename\n",
       "      * Bytestring containing raw PCM data or\n",
       "      * URL pointing to a file on the web.\n",
       "\n",
       "    If the array option is used, the waveform will be normalized.\n",
       "\n",
       "    If a filename or url is used, the format support will be browser\n",
       "    dependent.\n",
       "url : unicode\n",
       "    A URL to download the data from.\n",
       "filename : unicode\n",
       "    Path to a local file to load the data from.\n",
       "embed : boolean\n",
       "    Should the audio data be embedded using a data URI (True) or should\n",
       "    the original source be referenced. Set this to True if you want the\n",
       "    audio to playable later with no internet connection in the notebook.\n",
       "\n",
       "    Default is `True`, unless the keyword argument `url` is set, then\n",
       "    default value is `False`.\n",
       "rate : integer\n",
       "    The sampling rate of the raw data.\n",
       "    Only required when data parameter is being used as an array\n",
       "autoplay : bool\n",
       "    Set to True if the audio should immediately start playing.\n",
       "    Default is `False`.\n",
       "normalize : bool\n",
       "    Whether audio should be normalized (rescaled) to the maximum possible\n",
       "    range. Default is `True`. When set to `False`, `data` must be between\n",
       "    -1 and 1 (inclusive), otherwise an error is raised.\n",
       "    Applies only when `data` is a list or array of samples; other types of\n",
       "    audio are never normalized.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "\n",
       ">>> import pytest\n",
       ">>> np = pytest.importorskip(\"numpy\")\n",
       "\n",
       "Generate a sound\n",
       "\n",
       ">>> import numpy as np\n",
       ">>> framerate = 44100\n",
       ">>> t = np.linspace(0,5,framerate*5)\n",
       ">>> data = np.sin(2*np.pi*220*t) + np.sin(2*np.pi*224*t)\n",
       ">>> Audio(data, rate=framerate)\n",
       "<IPython.lib.display.Audio object>\n",
       "\n",
       "Can also do stereo or more channels\n",
       "\n",
       ">>> dataleft = np.sin(2*np.pi*220*t)\n",
       ">>> dataright = np.sin(2*np.pi*224*t)\n",
       ">>> Audio([dataleft, dataright], rate=framerate)\n",
       "<IPython.lib.display.Audio object>\n",
       "\n",
       "From URL:\n",
       "\n",
       ">>> Audio(\"http://www.nch.com.au/acm/8k16bitpcm.wav\")  # doctest: +SKIP\n",
       ">>> Audio(url=\"http://www.w3schools.com/html/horse.ogg\")  # doctest: +SKIP\n",
       "\n",
       "From a File:\n",
       "\n",
       ">>> Audio('IPython/lib/tests/test.wav')  # doctest: +SKIP\n",
       ">>> Audio(filename='IPython/lib/tests/test.wav')  # doctest: +SKIP\n",
       "\n",
       "From Bytes:\n",
       "\n",
       ">>> Audio(b'RAW_WAV_DATA..')  # doctest: +SKIP\n",
       ">>> Audio(data=b'RAW_WAV_DATA..')  # doctest: +SKIP\n",
       "\n",
       "See Also\n",
       "--------\n",
       "ipywidgets.Audio\n",
       "\n",
       "     Audio widget with more more flexibility and options.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Create a display object given raw data.\n",
       "\n",
       "When this object is returned by an expression or passed to the\n",
       "display function, it will result in the data being displayed\n",
       "in the frontend. The MIME type of the data should match the\n",
       "subclasses used, so the Png subclass should be used for 'image/png'\n",
       "data. If the data is a URL, the data will first be downloaded\n",
       "and then displayed. If\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data : unicode, str or bytes\n",
       "    The raw data or a URL or file to load the data from\n",
       "url : unicode\n",
       "    A URL to download the data from.\n",
       "filename : unicode\n",
       "    Path to a local file to load the data from.\n",
       "metadata : dict\n",
       "    Dict of metadata associated to be the object when displayed\n",
       "\u001b[0;31mFile:\u001b[0m           /opt/conda/lib/python3.10/site-packages/IPython/lib/display.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Audio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e5ef2d-46fa-4b25-8812-26262eead7be",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Audio.__init__() got an unexpected keyword argument 'sampling_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m common_voice \u001b[38;5;241m=\u001b[39m common_voice\u001b[38;5;241m.\u001b[39mcast_column(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mAudio\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Audio.__init__() got an unexpected keyword argument 'sampling_rate'"
     ]
    }
   ],
   "source": [
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a631c745-7e98-486d-856b-998240104fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "\n",
    "    example = processor(\n",
    "        audio=audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        text=example[\"sentence\"],\n",
    "    )\n",
    "\n",
    "    # compute input length of audio sample in seconds\n",
    "    example[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd612f-622c-41b3-933f-d90dc3d1b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice = common_voice.map(\n",
    "    prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97348973-57d4-4511-8992-8ad20de96053",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 30.0\n",
    "\n",
    "\n",
    "def is_audio_in_length_range(length):\n",
    "    return length < max_input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3c1d76-abc8-4529-addc-1e50f386b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice[\"train\"] = common_voice[\"train\"].filter(\n",
    "    is_audio_in_length_range,\n",
    "    input_columns=[\"input_length\"],\n",
    ")\n",
    "common_voice[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71581370-819f-4c58-9fa7-d0e5942de9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(\n",
    "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [\n",
    "            {\"input_features\": feature[\"input_features\"][0]} for feature in features\n",
    "        ]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3675be8-8bc9-4620-a654-61f409af04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d1fc5c-7427-42c1-9213-336e368bded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # compute orthographic wer\n",
    "    wer_ortho = 100 * wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    # compute normalised WER\n",
    "    pred_str_norm = [normalizer(pred) for pred in pred_str]\n",
    "    label_str_norm = [normalizer(label) for label in label_str]\n",
    "    # filtering step to only evaluate the samples that correspond to non-zero references:\n",
    "    pred_str_norm = [\n",
    "        pred_str_norm[i] for i in range(len(pred_str_norm)) if len(label_str_norm[i]) > 0\n",
    "    ]\n",
    "    label_str_norm = [\n",
    "        label_str_norm[i]\n",
    "        for i in range(len(label_str_norm))\n",
    "        if len(label_str_norm[i]) > 0\n",
    "    ]\n",
    "\n",
    "    wer = 100 * wer_metric.compute(predictions=pred_str_norm, references=label_str_norm)\n",
    "\n",
    "    return {\"wer_ortho\": wer_ortho, \"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ec71a6-11ea-45cd-8e48-f9f469fda21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3490228c-9d4b-4f9b-9def-dc5f2717b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable cache during training since it's incompatible with gradient checkpointing\n",
    "model.config.use_cache = False\n",
    "\n",
    "# set language and task for generation and re-enable cache\n",
    "model.generate = partial(\n",
    "    model.generate, language=\"sinhalese\", task=\"transcribe\", use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea0d78-7809-4c90-8e9c-8e106e418694",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./hfa-whisper-tiny-dv\",  # name on the HF Hub\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    warmup_steps=50,\n",
    "    max_steps=500,  # increase to 4000 if you have your own GPU or a Colab paid plan\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e2334-67d4-4a58-8bd6-dcbb628243fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9b4049-a278-497e-923d-00dd111bc9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fdbfe5-c842-42fb-912b-6cc4f11d3e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"dataset_tags\": \"mozilla-foundation/common_voice_13_0\",\n",
    "    \"dataset\": \"Common Voice 13\",  # a 'pretty' name for the training dataset\n",
    "    \"language\": \"dv\",\n",
    "    \"model_name\": \"hf_audio_course Whisper tiny Dv learning\",  # a 'pretty' name for your model\n",
    "    \"finetuned_from\": \"openai/whisper-tiny\",\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "}\n",
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002ae0e-6243-401d-90e0-766f58f16808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
