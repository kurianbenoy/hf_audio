{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5d0ed6c-7284-4c81-bacb-e3945dcac91b",
   "metadata": {},
   "source": [
    "# Unit-5 Huggingface Audio course\n",
    "\n",
    "Building Dhievi fine-tuned whisper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38c1228-9d93-49aa-aaf1-456e3a70470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -Uqq transformers evaluate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f97ac-5e95-461d-8d45-12af17aa9cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/kurianbenoy/mambaforge/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1093, in _get_module\n",
      "    def __reduce__(self):\n",
      "  File \"/home/kurianbenoy/mambaforge/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/home/kurianbenoy/mambaforge/lib/python3.10/site-packages/transformers/models/whisper/processing_whisper.py\", line 20, in <module>\n",
      "    from ...processing_utils import ProcessorMixin\n",
      "  File \"/home/kurianbenoy/mambaforge/lib/python3.10/site-packages/transformers/processing_utils.py\", line 24, in <module>\n",
      "    from .utils import PushToHubMixin, copy_func, direct_transformers_import, logging\n",
      "ImportError: cannot import name 'direct_transformers_import' from 'transformers.utils' (/home/kurianbenoy/mambaforge/lib/python3.10/site-packages/transformers/utils/__init__.py)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kurianbenoy/mambaforge/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_975/1801086296.py\", line 15, in <module>\n",
      "    from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments\n",
      "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
      "  File \"/home/kurianbenoy/mambaforge/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1084, in __getattr__\n",
      "    def _get_module(self, module_name: str):\n",
      "  File \"/home/kurianbenoy/mambaforge/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1083, in __getattr__\n",
      "  File \"/home/kurianbenoy/mambaforge/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1095, in _get_module\n",
      "RuntimeError: Failed to import transformers.models.whisper.processing_whisper because of the following error (look up to see its traceback):\n",
      "cannot import name 'direct_transformers_import' from 'transformers.utils' (/home/kurianbenoy/mambaforge/lib/python3.10/site-packages/transformers/utils/__init__.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kurianbenoy/mambaforge/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2052, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/kurianbenoy/mambaforge/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/kurianbenoy/mambaforge/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/kurianbenoy/mambaforge/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/kurianbenoy/mambaforge/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/home/kurianbenoy/mambaforge/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/home/kurianbenoy/mambaforge/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/kurianbenoy/mambaforge/lib/python3.10/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/kurianbenoy/mambaforge/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/kurianbenoy/mambaforge/lib/python3.10/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/home/kurianbenoy/mambaforge/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/kurianbenoy/mambaforge/lib/python3.10/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/home/kurianbenoy/mambaforge/lib/python3.10/site-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "from datasets import load_dataset, DatasetDict, Audio\n",
    "from evaluate import load\n",
    "from IPython.display import Audio\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "from transformers.models.whisper.tokenization_whisper import TO_LANGUAGE_CODE\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2586647d-f8a0-4ba9-be99-545bff9900c5",
   "metadata": {},
   "source": [
    "## datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c8b277-9a5a-4a48-888d-e9d7c2c7d7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_test = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e6142c-d497-4397-b605-2e055836c6db",
   "metadata": {},
   "source": [
    "## Native performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c7a0be-ac35-4073-b30e-a8f12c163de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    torch_dtype = torch.float16\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    torch_dtype = torch.float32\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-small\",\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cf37a6-da62-4753-b2b0-f5a9f5d05dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "\n",
    "# run streamed inference\n",
    "for prediction in tqdm(\n",
    "    pipe(\n",
    "        KeyDataset(common_voice_test, \"audio\"),\n",
    "        max_new_tokens=128,\n",
    "        generate_kwargs={\"task\": \"transcribe\"},\n",
    "        batch_size=32,\n",
    "    ),\n",
    "    total=len(common_voice_test),\n",
    "):\n",
    "    all_predictions.append(prediction[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536fa47e-ab3a-4076-8698-a54ccfddab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463a1251-d328-4b01-9749-8513bfa3133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_ortho = 100 * wer_metric.compute(\n",
    "    references=common_voice_test[\"sentence\"], predictions=all_predictions\n",
    ")\n",
    "wer_ortho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86028c5a-9167-4113-a2b6-23636532dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = BasicTextNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c25a09-caad-422c-8f05-b633ac19b706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute normalised WER\n",
    "all_predictions_norm = [normalizer(pred) for pred in all_predictions]\n",
    "all_references_norm = [normalizer(label) for label in common_voice_test[\"sentence\"]]\n",
    "\n",
    "# filtering step to only evaluate the samples that correspond to non-zero references\n",
    "all_predictions_norm = [\n",
    "    all_predictions_norm[i]\n",
    "    for i in range(len(all_predictions_norm))\n",
    "    if len(all_references_norm[i]) > 0\n",
    "]\n",
    "all_references_norm = [\n",
    "    all_references_norm[i]\n",
    "    for i in range(len(all_references_norm))\n",
    "    if len(all_references_norm[i]) > 0\n",
    "]\n",
    "\n",
    "wer = 100 * wer_metric.compute(\n",
    "    references=all_references_norm, predictions=all_predictions_norm\n",
    ")\n",
    "\n",
    "wer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec5650-23e5-4f5a-9090-3b3be75975fc",
   "metadata": {},
   "source": [
    "## Fine-tuning whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6624f1b8-f730-4505-9ffb-7b9023da7b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"train+validation\"\n",
    ")\n",
    "common_voice[\"test\"] = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"test\"\n",
    ")\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03ea9d0-fd95-4f8c-8b57-d972eeec471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice = common_voice.select_columns([\"audio\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e3a227-6cdc-4b6d-bbba-d721cf06fc92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': 'en',\n",
       " 'chinese': 'zh',\n",
       " 'german': 'de',\n",
       " 'spanish': 'es',\n",
       " 'russian': 'ru',\n",
       " 'korean': 'ko',\n",
       " 'french': 'fr',\n",
       " 'japanese': 'ja',\n",
       " 'portuguese': 'pt',\n",
       " 'turkish': 'tr',\n",
       " 'polish': 'pl',\n",
       " 'catalan': 'ca',\n",
       " 'dutch': 'nl',\n",
       " 'arabic': 'ar',\n",
       " 'swedish': 'sv',\n",
       " 'italian': 'it',\n",
       " 'indonesian': 'id',\n",
       " 'hindi': 'hi',\n",
       " 'finnish': 'fi',\n",
       " 'vietnamese': 'vi',\n",
       " 'hebrew': 'he',\n",
       " 'ukrainian': 'uk',\n",
       " 'greek': 'el',\n",
       " 'malay': 'ms',\n",
       " 'czech': 'cs',\n",
       " 'romanian': 'ro',\n",
       " 'danish': 'da',\n",
       " 'hungarian': 'hu',\n",
       " 'tamil': 'ta',\n",
       " 'norwegian': 'no',\n",
       " 'thai': 'th',\n",
       " 'urdu': 'ur',\n",
       " 'croatian': 'hr',\n",
       " 'bulgarian': 'bg',\n",
       " 'lithuanian': 'lt',\n",
       " 'latin': 'la',\n",
       " 'maori': 'mi',\n",
       " 'malayalam': 'ml',\n",
       " 'welsh': 'cy',\n",
       " 'slovak': 'sk',\n",
       " 'telugu': 'te',\n",
       " 'persian': 'fa',\n",
       " 'latvian': 'lv',\n",
       " 'bengali': 'bn',\n",
       " 'serbian': 'sr',\n",
       " 'azerbaijani': 'az',\n",
       " 'slovenian': 'sl',\n",
       " 'kannada': 'kn',\n",
       " 'estonian': 'et',\n",
       " 'macedonian': 'mk',\n",
       " 'breton': 'br',\n",
       " 'basque': 'eu',\n",
       " 'icelandic': 'is',\n",
       " 'armenian': 'hy',\n",
       " 'nepali': 'ne',\n",
       " 'mongolian': 'mn',\n",
       " 'bosnian': 'bs',\n",
       " 'kazakh': 'kk',\n",
       " 'albanian': 'sq',\n",
       " 'swahili': 'sw',\n",
       " 'galician': 'gl',\n",
       " 'marathi': 'mr',\n",
       " 'punjabi': 'pa',\n",
       " 'sinhala': 'si',\n",
       " 'khmer': 'km',\n",
       " 'shona': 'sn',\n",
       " 'yoruba': 'yo',\n",
       " 'somali': 'so',\n",
       " 'afrikaans': 'af',\n",
       " 'occitan': 'oc',\n",
       " 'georgian': 'ka',\n",
       " 'belarusian': 'be',\n",
       " 'tajik': 'tg',\n",
       " 'sindhi': 'sd',\n",
       " 'gujarati': 'gu',\n",
       " 'amharic': 'am',\n",
       " 'yiddish': 'yi',\n",
       " 'lao': 'lo',\n",
       " 'uzbek': 'uz',\n",
       " 'faroese': 'fo',\n",
       " 'haitian creole': 'ht',\n",
       " 'pashto': 'ps',\n",
       " 'turkmen': 'tk',\n",
       " 'nynorsk': 'nn',\n",
       " 'maltese': 'mt',\n",
       " 'sanskrit': 'sa',\n",
       " 'luxembourgish': 'lb',\n",
       " 'myanmar': 'my',\n",
       " 'tibetan': 'bo',\n",
       " 'tagalog': 'tl',\n",
       " 'malagasy': 'mg',\n",
       " 'assamese': 'as',\n",
       " 'tatar': 'tt',\n",
       " 'hawaiian': 'haw',\n",
       " 'lingala': 'ln',\n",
       " 'hausa': 'ha',\n",
       " 'bashkir': 'ba',\n",
       " 'javanese': 'jw',\n",
       " 'sundanese': 'su',\n",
       " 'burmese': 'my',\n",
       " 'valencian': 'ca',\n",
       " 'flemish': 'nl',\n",
       " 'haitian': 'ht',\n",
       " 'letzeburgesch': 'lb',\n",
       " 'pushto': 'ps',\n",
       " 'panjabi': 'pa',\n",
       " 'moldavian': 'ro',\n",
       " 'moldovan': 'ro',\n",
       " 'sinhalese': 'si',\n",
       " 'castilian': 'es'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TO_LANGUAGE_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f28fcc2-3607-4c8a-a5b0-5fa7b7768798",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\n",
    "    \"openai/whisper-small\", language=\"sinhalese\", task=\"transcribe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e4ab46-02c6-4694-90c9-3b31fa58c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b522f3-427e-43a3-a015-6d7d69588892",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = processor.feature_extractor.sampling_rate\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a631c745-7e98-486d-856b-998240104fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "\n",
    "    example = processor(\n",
    "        audio=audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        text=example[\"sentence\"],\n",
    "    )\n",
    "\n",
    "    # compute input length of audio sample in seconds\n",
    "    example[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd612f-622c-41b3-933f-d90dc3d1b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice = common_voice.map(\n",
    "    prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97348973-57d4-4511-8992-8ad20de96053",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 30.0\n",
    "\n",
    "\n",
    "def is_audio_in_length_range(length):\n",
    "    return length < max_input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3c1d76-abc8-4529-addc-1e50f386b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice[\"train\"] = common_voice[\"train\"].filter(\n",
    "    is_audio_in_length_range,\n",
    "    input_columns=[\"input_length\"],\n",
    ")\n",
    "common_voice[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71581370-819f-4c58-9fa7-d0e5942de9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(\n",
    "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [\n",
    "            {\"input_features\": feature[\"input_features\"][0]} for feature in features\n",
    "        ]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3675be8-8bc9-4620-a654-61f409af04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d1fc5c-7427-42c1-9213-336e368bded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # compute orthographic wer\n",
    "    wer_ortho = 100 * wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    # compute normalised WER\n",
    "    pred_str_norm = [normalizer(pred) for pred in pred_str]\n",
    "    label_str_norm = [normalizer(label) for label in label_str]\n",
    "    # filtering step to only evaluate the samples that correspond to non-zero references:\n",
    "    pred_str_norm = [\n",
    "        pred_str_norm[i] for i in range(len(pred_str_norm)) if len(label_str_norm[i]) > 0\n",
    "    ]\n",
    "    label_str_norm = [\n",
    "        label_str_norm[i]\n",
    "        for i in range(len(label_str_norm))\n",
    "        if len(label_str_norm[i]) > 0\n",
    "    ]\n",
    "\n",
    "    wer = 100 * wer_metric.compute(predictions=pred_str_norm, references=label_str_norm)\n",
    "\n",
    "    return {\"wer_ortho\": wer_ortho, \"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ec71a6-11ea-45cd-8e48-f9f469fda21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3490228c-9d4b-4f9b-9def-dc5f2717b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable cache during training since it's incompatible with gradient checkpointing\n",
    "model.config.use_cache = False\n",
    "\n",
    "# set language and task for generation and re-enable cache\n",
    "model.generate = partial(\n",
    "    model.generate, language=\"sinhalese\", task=\"transcribe\", use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea0d78-7809-4c90-8e9c-8e106e418694",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-dv\",  # name on the HF Hub\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    warmup_steps=50,\n",
    "    max_steps=500,  # increase to 4000 if you have your own GPU or a Colab paid plan\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e2334-67d4-4a58-8bd6-dcbb628243fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9b4049-a278-497e-923d-00dd111bc9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fdbfe5-c842-42fb-912b-6cc4f11d3e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"dataset_tags\": \"mozilla-foundation/common_voice_13_0\",\n",
    "    \"dataset\": \"Common Voice 13\",  # a 'pretty' name for the training dataset\n",
    "    \"language\": \"dv\",\n",
    "    \"model_name\": \"hf_audio_course Whisper Small Dv learning\",  # a 'pretty' name for your model\n",
    "    \"finetuned_from\": \"openai/whisper-small\",\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "}\n",
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002ae0e-6243-401d-90e0-766f58f16808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
